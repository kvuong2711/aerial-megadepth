# Aerial-MegaDepth Data Generation

This repository provides a dataset and pipeline for generating pseudo-synthetic multi-view image collections using Google Earth and MegaDepth. It includes pre-rendered samples as well as instructions for generating your own data from scratch.

> **Disclaimer**: Due to licensing restrictions, we do not redistribute Google Earth or MegaDepth images in bulk. Instead, we provide a minimal example and tools to reproduce the full dataset.

## ğŸ“¦ Sample Data

We provide a sample scene (`0001`) to illustrate the format and structure of the dataset. You can download it directly using the AWS CLI.

### Download via CLI

First, install the [AWS CLI](https://aws.amazon.com/cli/) if you havenâ€™t already. Then run:

```bash
mkdir -p megadepth_aerial_data/data
aws s3 sync s3://aerial-megadepth/full_data/0001 megadepth_aerial_data/data/0001
```
This command will download the sample scene data to `megadepth_aerial_data/data/0001`.

### Sample Data Structure

```
megadepth_aerial_tv/
â””â”€â”€ data/
    â””â”€â”€ 0001/
        â””â”€â”€ sfm_output_localization/
            â””â”€â”€ sfm_superpoint+superglue/
                â””â”€â”€ localized_dense_metric/
                    â”œâ”€â”€ images/           # RGB images (Google Earth & MegaDepth)
                    â”œâ”€â”€ depths/           # Depth maps
                    â””â”€â”€ sparse-txt/       # COLMAP reconstruction files
```

## ğŸ› ï¸ Generating Data from Scratch

The full pipeline involves two stages:

1. [Generating Pseudo-Synthetic Data](#1-generating-pseudo-synthetic-data)  
2. [Registering to MegaDepth](#2-registering-to-megadepth)

### 1ï¸âƒ£ Generating Pseudo-Synthetic Data from Google Earth Studio

This stage creates video frames and camera metadata using Google Earth Studio.

#### Step 1: Render Using Google Earth Studio

Each scene comes with pre-defined camera parameters in `.esp` format. You can download all `.esp` files using:

```bash
aws s3 sync s3://aerial-megadepth/geojsons ./megadepth_aerial_tv/geojsons
```

Directory structure:

```
megadepth_aerial_tv/
â””â”€â”€ geojsons/
    â”œâ”€â”€ 0001/
    â”‚   â””â”€â”€ 0001.esp
    â”œâ”€â”€ 0002/
    â”‚   â””â”€â”€ 0002.esp
    â””â”€â”€ ...
```

To render the pseudo-synthetic sequence:

1. Open [Google Earth Studio](https://earth.google.com/studio/)
2. Import a `.esp` file via **File â†’ Import â†’ Earth Studio Project**
3. Go to **Render** and export:
   - **Video**: select **Cloud Rendering** to produce a `.mp4`
   - **Tracking Data**: enable **3D Camera Tracking (JSON)** with **Coordinate Space: Global**

Save the exported files to:

```
megadepth_aerial_tv/
â””â”€â”€ downloaded_data/
    â”œâ”€â”€ 0001.mp4     # Rendered video
    â”œâ”€â”€ 0001.json    # Camera metadata (pose, intrinsics, timestamps)
    â””â”€â”€ ...
```

> ğŸ’¡ **Note:** This step requires manual interaction with Google Earth Studio which is a bit inconvenient. Therefore, we actively welcome [pull requests](https://github.com/your-repo-url) or discussions that help automate this step or streamline rendering workflows.

#### Step 2: Extract Frames & Align Metadata

Use a preprocessing script to extract video frames and organize the data structure like this:
#### TODO: add the script

```
megadepth_aerial_tv/
â””â”€â”€ data/
    â”œâ”€â”€ 0001/
    â”‚   â”œâ”€â”€ 0001.json               # camera parameters
    â”‚   â””â”€â”€ footage/
    â”‚       â”œâ”€â”€ frame_000000.jpeg
    â”‚       â”œâ”€â”€ frame_000001.jpeg
    â”‚       â””â”€â”€ ...
    â””â”€â”€ ...
```
---

### 2ï¸âƒ£ Registering to MegaDepth

Once pseudo-synthetic images are generated, the next step is to localize them in a MegaDepth scene and reconstruct the geometry.

#### Step 1: Prepare MegaDepth Images

Use the provided preprocessing script to extract images, depths, and camera poses from a MegaDepth scene. The processed files are saved in the following format:

```text
megadepth_processed_extra/
â”œâ”€â”€ 0001/
â”‚   â””â”€â”€ 0/
â”‚       â”œâ”€â”€ 5008984_74a994ce1c_o.jpg.npz    # Camera pose + intrinsics
â”‚       â”œâ”€â”€ 5008984_74a994ce1c_o.jpg.exr    # Depth map (EXR format)
â”‚       â”œâ”€â”€ 5008984_74a994ce1c_o.jpg.jpg    # RGB image
â”‚       â””â”€â”€ ...
â”œâ”€â”€ 0002/
â”‚   â””â”€â”€ 0/
â”‚       â””â”€â”€ ...
â””â”€â”€ ...
```

Each `.jpg` file has a matching `.npz` (camera parameters) and `.exr` (depth map).

---

#### Step 2: Run the Data Generation Pipeline

With both pseudo-synthetic and MegaDepth data prepared, you can run the localization and reconstruction pipeline (e.g., SuperPoint + SuperGlue + COLMAP). The output is saved per scene as:

```text
megadepth_aerial_tv/
â””â”€â”€ data/
    â””â”€â”€ 0001/
        â””â”€â”€ sfm_output_localization/
            â””â”€â”€ sfm_superpoint+superglue/
                â””â”€â”€ localized_dense_metric/
                    â”œâ”€â”€ images/           # Registered RGB images
                    â”œâ”€â”€ depths/           # Optional MVS depth maps
                    â””â”€â”€ sparse-txt/       # COLMAP poses + intrinsics (text format)
```

---

Let me know if you'd like this split into a checklist or numbered steps, or if you'd like help formatting the CLI commands for running each stage.